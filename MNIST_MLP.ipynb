{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mnist_test.csv', 'mnist_test.csv.zip', 'mnist_train.csv', 'mnist_train.csv.zip']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code borrowed from: \"Python Machine Learning\" (2nd Edition) by Sebastian Raschka and Vahid Mirjalili\n",
    "Dataset borrowed from: https://www.kaggle.com/oddrationale/mnist-in-csv\n",
    "\"\"\"\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "# import warnings\n",
    "import warnings\n",
    "# filter warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"C:/Books/Texas State Books/MNIST\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read train \n",
    "train = pd.read_csv(\"C:/Books/Texas State Books/MNIST/mnist_train.csv\")\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read test \n",
    "test= pd.read_csv(\"C:/Books/Texas State Books/MNIST/mnist_test.csv\")\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# put labels into y_train variable\n",
    "y_train = train[\"label\"]\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1)\n",
    "\n",
    "# put labels into y_test variable\n",
    "y_test = test[\"label\"]\n",
    "# Drop 'label' column\n",
    "X_test = test.drop(labels = [\"label\"],axis = 1)\n",
    "\n",
    "# Normalize the data\n",
    "X_train_centered = X_train / 255.0\n",
    "X_test_centered = X_test / 255.0\n",
    "del X_train, X_test\n",
    "\n",
    "print(X_train_centered.shape, y_train.shape)\n",
    "print(X_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seeds for reproducibility\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# One-Hot Encode the classes\n",
    "y_train_onehot = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense( units=50,\n",
    "        input_dim=X_train_centered.shape[1],\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense( units=50,\n",
    "        input_dim=50,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh'))\n",
    "\n",
    "model.add(\n",
    "    keras.layers.Dense( units=y_train_onehot.shape[1],\n",
    "        input_dim=50,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "54000/54000 [==============================] - 3s 60us/sample - loss: 0.9660 - val_loss: 0.4692\n",
      "Epoch 2/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.4600 - val_loss: 0.3363\n",
      "Epoch 3/50\n",
      "54000/54000 [==============================] - 2s 45us/sample - loss: 0.3732 - val_loss: 0.2876\n",
      "Epoch 4/50\n",
      "54000/54000 [==============================] - 2s 45us/sample - loss: 0.3314 - val_loss: 0.2601\n",
      "Epoch 5/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.3045 - val_loss: 0.2409\n",
      "Epoch 6/50\n",
      "54000/54000 [==============================] - 2s 45us/sample - loss: 0.2845 - val_loss: 0.2287\n",
      "Epoch 7/50\n",
      "54000/54000 [==============================] - 3s 52us/sample - loss: 0.2681 - val_loss: 0.2136\n",
      "Epoch 8/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.2539 - val_loss: 0.2039\n",
      "Epoch 9/50\n",
      "54000/54000 [==============================] - 3s 47us/sample - loss: 0.2415 - val_loss: 0.1941\n",
      "Epoch 10/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.2304 - val_loss: 0.1849\n",
      "Epoch 11/50\n",
      "54000/54000 [==============================] - 2s 45us/sample - loss: 0.2200 - val_loss: 0.1785\n",
      "Epoch 12/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.2108 - val_loss: 0.1710\n",
      "Epoch 13/50\n",
      "54000/54000 [==============================] - 3s 51us/sample - loss: 0.2020 - val_loss: 0.1657\n",
      "Epoch 14/50\n",
      "54000/54000 [==============================] - 3s 47us/sample - loss: 0.1942 - val_loss: 0.1586\n",
      "Epoch 15/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1869 - val_loss: 0.1545\n",
      "Epoch 16/50\n",
      "54000/54000 [==============================] - 2s 45us/sample - loss: 0.1802 - val_loss: 0.1500\n",
      "Epoch 17/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1739 - val_loss: 0.1447\n",
      "Epoch 18/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1680 - val_loss: 0.1405\n",
      "Epoch 19/50\n",
      "54000/54000 [==============================] - 3s 48us/sample - loss: 0.1627 - val_loss: 0.1371\n",
      "Epoch 20/50\n",
      "54000/54000 [==============================] - 3s 51us/sample - loss: 0.1574 - val_loss: 0.1346\n",
      "Epoch 21/50\n",
      "54000/54000 [==============================] - 3s 47us/sample - loss: 0.1528 - val_loss: 0.1320\n",
      "Epoch 22/50\n",
      "54000/54000 [==============================] - 3s 47us/sample - loss: 0.1483 - val_loss: 0.1278\n",
      "Epoch 23/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1439 - val_loss: 0.1248\n",
      "Epoch 24/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1402 - val_loss: 0.1224\n",
      "Epoch 25/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1364 - val_loss: 0.1197\n",
      "Epoch 26/50\n",
      "54000/54000 [==============================] - 3s 52us/sample - loss: 0.1328 - val_loss: 0.1187\n",
      "Epoch 27/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1295 - val_loss: 0.1163\n",
      "Epoch 28/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1263 - val_loss: 0.1142\n",
      "Epoch 29/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1232 - val_loss: 0.1131\n",
      "Epoch 30/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1203 - val_loss: 0.1109\n",
      "Epoch 31/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1175 - val_loss: 0.1089\n",
      "Epoch 32/50\n",
      "54000/54000 [==============================] - 3s 53us/sample - loss: 0.1148 - val_loss: 0.1074\n",
      "Epoch 33/50\n",
      "54000/54000 [==============================] - 3s 47us/sample - loss: 0.1122 - val_loss: 0.1061\n",
      "Epoch 34/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1097 - val_loss: 0.1053\n",
      "Epoch 35/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1074 - val_loss: 0.1034\n",
      "Epoch 36/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1050 - val_loss: 0.1033\n",
      "Epoch 37/50\n",
      "54000/54000 [==============================] - 2s 46us/sample - loss: 0.1030 - val_loss: 0.1019\n",
      "Epoch 38/50\n",
      "54000/54000 [==============================] - 3s 50us/sample - loss: 0.1009 - val_loss: 0.0998\n",
      "Epoch 39/50\n",
      "54000/54000 [==============================] - 3s 51us/sample - loss: 0.0987 - val_loss: 0.0994\n",
      "Epoch 40/50\n",
      "54000/54000 [==============================] - 3s 50us/sample - loss: 0.0970 - val_loss: 0.0987\n",
      "Epoch 41/50\n",
      "54000/54000 [==============================] - 3s 61us/sample - loss: 0.0949 - val_loss: 0.0992\n",
      "Epoch 42/50\n",
      "54000/54000 [==============================] - 4s 72us/sample - loss: 0.0930 - val_loss: 0.0978\n",
      "Epoch 43/50\n",
      "54000/54000 [==============================] - 3s 57us/sample - loss: 0.0913 - val_loss: 0.0955\n",
      "Epoch 44/50\n",
      "54000/54000 [==============================] - 3s 55us/sample - loss: 0.0897 - val_loss: 0.0945\n",
      "Epoch 45/50\n",
      "54000/54000 [==============================] - 3s 58us/sample - loss: 0.0881 - val_loss: 0.0930\n",
      "Epoch 46/50\n",
      "54000/54000 [==============================] - 3s 56us/sample - loss: 0.0863 - val_loss: 0.0927\n",
      "Epoch 47/50\n",
      "54000/54000 [==============================] - 3s 52us/sample - loss: 0.0849 - val_loss: 0.0933\n",
      "Epoch 48/50\n",
      "54000/54000 [==============================] - 3s 50us/sample - loss: 0.0834 - val_loss: 0.0923\n",
      "Epoch 49/50\n",
      "54000/54000 [==============================] - 3s 58us/sample - loss: 0.0819 - val_loss: 0.0910\n",
      "Epoch 50/50\n",
      "54000/54000 [==============================] - 3s 50us/sample - loss: 0.0803 - val_loss: 0.0911\n"
     ]
    }
   ],
   "source": [
    "sgd_optimizer = keras.optimizers.SGD(\n",
    "                    lr=0.001, decay=1e-7, momentum=.9)\n",
    "\n",
    "model.compile(optimizer=sgd_optimizer,\n",
    "    loss='categorical_crossentropy')\n",
    "\n",
    "# Train the ANN\n",
    "history = model.fit(X_train_centered, y_train_onehot,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    verbose=1,\n",
    "    validation_split=0.1) # 90% training / 10% validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 97.76%\n",
      "Test accuracy: 96.80%\n"
     ]
    }
   ],
   "source": [
    "# Print the performance metrics\n",
    "y_train_pred = model.predict_classes(X_train_centered, verbose=0)\n",
    "correct_preds = np.sum(y_train == y_train_pred, axis=0)\n",
    "train_acc = correct_preds / y_train.shape[0]\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (train_acc * 100))\n",
    "\n",
    "y_test_pred = model.predict_classes(X_test_centered, verbose=0)\n",
    "correct_preds = np.sum(y_test == y_test_pred, axis=0)\n",
    "test_acc = correct_preds / y_test.shape[0]\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
